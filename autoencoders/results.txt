#encoder
conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img) #28 x 28 x 32
batch1 = BatchNormalization()(conv1)
pool1 = MaxPooling2D(pool_size=(2, 2))(batch1) #14 x 14 x 32
conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1) #14 x 14 x 64
batch2 = BatchNormalization()(conv2)
pool2 = MaxPooling2D(pool_size=(2, 2))(batch2) #7 x 7 x 64
conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2) #7 x 7 x 128 (small and thick)

#decoder
conv4 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3) #7 x 7 x 128
batch4 = BatchNormalization()(conv4)
up1 = UpSampling2D((2,2))(batch4) # 14 x 14 x 128
conv5 = Conv2D(64, (3, 3), activation='relu', padding='same')(up1) # 14 x 14 x 64
batch5 = BatchNormalization()(conv5)
up2 = UpSampling2D((2,2))(batch5) # 28 x 28 x 64
decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(up2) # 28 x 28 x 1

RESULT: loss: 8*10-4, 320k params, 48k samples, 1 for 10 classes
